{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/apple/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from gensim.models import Word2Vec\n",
    "import nltk\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "\n",
    "# 1. Load cleaned data\n",
    "df = pd.read_csv(\"../data/CaptstoneProjectData_2025_cleaned.csv\")\n",
    "df[\"clean_text\"] = df[\"clean_text\"].fillna(\"\")\n",
    "\n",
    "# 2. Tokenize by splitting clean_text\n",
    "df[\"tokens\"] = df[\"clean_text\"].str.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Embedding - Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Train Word2Vec model\n",
    "w2v_model = Word2Vec(\n",
    "    sentences=df[\"tokens\"].tolist(),\n",
    "    vector_size=300,\n",
    "    window=5,\n",
    "    min_count=2,\n",
    "    workers=4,\n",
    "    epochs=10\n",
    ")\n",
    "\n",
    "# 4. Create document embedding by averaging word vectors\n",
    "def avg_embedding(tokens):\n",
    "    vectors = [w2v_model.wv[word] for word in tokens if word in w2v_model.wv]\n",
    "    return np.mean(vectors, axis=0) if vectors else np.zeros(w2v_model.vector_size)\n",
    "df[\"w2v_embedding\"] = df[\"tokens\"].apply(avg_embedding)\n",
    "\n",
    "# 5. Expand embeddings into separate columns\n",
    "w2v_cols = [f\"w2v_{i}\" for i in range(w2v_model.vector_size)]\n",
    "w2v_df = pd.DataFrame(df[\"w2v_embedding\"].tolist(), columns=w2v_cols)\n",
    "result = pd.concat([df.drop(columns=[\"w2v_embedding\"]), w2v_df], axis=1)\n",
    "\n",
    "# 6. Save to new CSV\n",
    "w2v_df.to_csv(\"../features/w2v_embeddings.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Embedding - BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87e2df57629a4c74b897834772f83caf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/81 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# 1. Load cleaned data\n",
    "df = pd.read_csv(\"../data/CaptstoneProjectData_2025_cleaned.csv\")\n",
    "texts = df[\"clean_text\"].tolist()\n",
    "\n",
    "# 2. Load pre-trained BERT model (SentenceTransformer)\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# 3. Compute embeddings for each email\n",
    "embeddings = model.encode(texts, batch_size=32, show_progress_bar=True)\n",
    "\n",
    "# 4. Expand embeddings into DataFrame\n",
    "bert_cols = [f\"bert_{i}\" for i in range(embeddings.shape[1])]\n",
    "bert_df = pd.DataFrame(embeddings, columns=bert_cols)\n",
    "result = pd.concat([df.reset_index(drop=True), bert_df], axis=1)\n",
    "\n",
    "# 5. Save to new CSV\n",
    "bert_df.to_csv(\"../features/bert_embeddings.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
